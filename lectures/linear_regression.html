<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>JHU 600.475 - Linear Regression</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="../reveal.js/css/reveal.css">
		<link rel="stylesheet" href="../reveal.js/css/theme/black.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="../reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Linear Regression</h1>
					<h3>CS 600.475</h3>
					<p>
						<small>Created by <a href="http://ericwb.me">Eric Bridgeford</a> / <a href="http://github.com/ebridge2">@ebridge2</a></small>
					</p>
				</section>

				<section>
					<section>
						<h1>What is a Linear Regression task?</h1>
					</section>
					<section>
						<h2>Simple Example</h2>
						<img src="../img/linear_task.png">
						<ul>
							<li>Here, $x \in \mathbb{R}^1$, $y \in \mathbb{R}^1$</li>
							<li>It looks like there is a general upwards "trend", but how do we find the best"trend"?</li>
							<li>What will our solution look like?</li>
						</ul>
					</section>

					<section>
						<h2>Equation of a Line</h2>
						<ul>
							<li>Your first thought might be to map that with something like, $y=mx+b$</li>
							<li>This works great for $1D$ examples, but our data is much more complex</li>
							<li>Redefine the "line" with linear algebra, and let</li>
							<li>$y = mx + b = \begin{bmatrix}1 & x\end{bmatrix}\begin{bmatrix} b \\ m \end{bmatrix}$</li>
						</ul>
					</section>

					<section>
						<h2>Some Slight Notational Adjustments</h2>
						<ul>
							<li>Let $x \in \mathbb{R}^{d+1}$: add $x_0 = 1$</li>
							<ul>
								<li>this is a NECESSARY switch (more on why later)</li>
								<li>$X = \begin{bmatrix} 1 & x_1 & ... & x_n\end{bmatrix}$</li>
							</ul>
							<li>Let $w = \begin{bmatrix} b \\ m \end{bmatrix} = \begin{bmatrix} w_0 \\ w_1 \end{bmatrix}$</li>
							<li>Let $Y = y$; capital since it is "technically" a matrix</li>
							<li>$Y = Xw$: the equation of a basic line</li>
							<li>$Y \in \mathbb{R}^{n \times 1}, X \in \mathbb{R}^{n \times d + 1}, w \in \mathbb{R}^{d+1 \times 1}$</li>
						</ul>
					</section>
				</section>

				<section>
					<section>
						<h2>Intuition</h2>
						<ul>
							<li>given a real valued input with some number of features, provide the "most-likely" real valued output</li>
							<li>Given: $\left\{(x_i, y_i)\right\}_{i=1}^n$, $x_i \in \mathbb{R}^d, y_i \in \mathbb{R}^d$ </li>
							<li>Model: $Y = Xw + ?$</li>
							<li>Can we find the weights for our model that minimize some sort of "loss" incurred on bad predictions?</li>
						</ul>
					</section>
					<section>
						<h2>Simple Example</h2>
						<img src="../img/linear_task.png", width="400">
						<ul>
							<li>Notice we could not fit this data with $Y=Xw$</li>
							<li>Why? Look at all the noise!</li>
							<li>Better model: $Y = Xw + \epsilon$</li>
							<li>$\epsilon \in \mathbb{R}^n$: all $n$ data points we are given have some error element</li>
						</ul>
					</section>
					<section>
						<h2>The Linear Model</h2>

						$Y = Xw + \epsilon$

						\[\begin{align*}
						\begin{bmatrix}
							y_1 \\ y_2 \\ \vdots \\ y_n
						\end{bmatrix}
						&=
						\begin{bmatrix}
							1 & x_{11} & x_{12} & ... & x_{1d} \\
							1 & x_{21} & x_{22} & ... & x_{2d} \\
							\vdots & \vdots & \vdots & \ddots & \vdots \\
							1 & x_{n1} & x_{n2} & ... & x_{nd} \\
						\end{bmatrix}
						\begin{bmatrix}
							w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_d
						\end{bmatrix}
						+
						\begin{bmatrix}
							\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n
						\end{bmatrix}

						\end{align*}\]
					</section>
				</section>

				<section>
					<section>
						<h1>LS Loss Function</h1>
					</section>
					<section>
						<h2>LS: Least Squares</h2>
						<ul>
							<li>Goes by many names: Ordinary Least Squares, Sum of Squared Residuals, Sum of Squared Error, etc.</li>
							<li>You pick; I personally like Sum of Squared Residuals (SSR); many people prefer Least Squares (course preferred)</li>
							<li>Disclaimer: the first time I saw this, I just zoned out because I got lost. When you look at the geometry, it is really quite simple though</li>
							<li>Try and bear with me, and let me know if something is unclear</li>
						</ul>
					</section>

					<section>
						<h2>What is a Residual?</h2>
						<ul>
							<li>A "residual" is just our error $\epsilon_i$ for a particular data point</li>
							<li>How much our prediction is "off" by</li>
							<li>Red is our data points, blue is our "estimate", green is our residual/error</li>
						</ul>
						<img src="../img/residual.png", width="300">
					</section>

					<section>
						<h2>Defining our Loss</h2>
						<ul>
							<li>If we have $Y = Xw + \epsilon$, then $\epsilon = Y - Xw$</li>
							<li>Loss function: $L(w | X, Y) = \textrm{min}_{w}f(\epsilon | X, Y)$</li>
							<li>$\hat{w} = \textrm{min}_{w}f(\epsilon | X, Y)$</li>
							<li>What function $f(\epsilon)$ should we pick to minimize the error?</li>
							<li>Remember our discussion on norms...</li>
						</ul>
					</section>

					<section>
						<h2>Finally, a Loss function!</h2>
						<ul>
							<li>let $f(\epsilon) = ||\epsilon||_2^2 = ||Y - Xw||_2^2$</li>
							<li>Our model becomes: $L(w | X, Y) = \textrm{argmin}_{w}||Y - Xw||_2^2$</li>
							<li>Pick the weights $w$ that minimize our error, or the difference between our observations $Y$ and our predictions $\hat{Y} = XW$</li>
							<li>How can we figure this out?</li>
						</ul>
					</section>
				</section>

				<section>
					<section>
						<h2>MV Calc to the rescue</h2>
						<ul>
							<li>Let's step back for a second and look at our model in terms of a matrix and iterative interpretation</li>
							<li>Remember the $L2$ norm: $\left|\left|x\right|\right|_2 = \sqrt{\sum_{i=1}^n x_i^2}$</li>
							<li>Here, we have $||\epsilon||_2^2 = \sum_{i=1}^n \epsilon_i^2$ (note the $\sqrt{\cdot}$ disappears!)</li>
							<li>This represents a link between our matrix representation, and a more intuitive representation going element by element</li>
						</ul>
					</section>

					<section>
						Note: $y_i \in \mathbb{R}$, $x_i \in \mathbb{R}^{d + 1}$, $w \in \mathbb{R}^{d+1}$
						\[\begin{align*}
							LS  = \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n (y_i - x_iw)^2 \\
							 &= \sum_{i=1}^n \left(y_i - \sum_{j=0}^d x_{ij}w_j\right)^2 \\
						\end{align*}\]
						<ul>
							<li>Minimize MV function: $\nabla_w LS = \begin{pmatrix} \frac{\delta LS}{\delta w_0} & ... & \frac{\delta LS}{\delta w_d} \end{pmatrix}=0$</li>
						</ul>
					</section>

					<section>
						Take a derivative wrt each element of our coefficients $w_k$
						\[\begin{align*}
						\frac{\delta LS}{\delta w_k}  &= \frac{\delta \sum_{i=1}^n \left(y_i - \sum_{j=0}^d x_{ij}w_j\right)^2}{\delta w_k} \\
						&= \sum_{i=1}^n -2x_{ik}\left(y_i - \sum_{j=0}^d x_{ij}w_j\right) \\
						&= \sum_{i=1}^n \left(-2x_{ik}y_i + 2x_{ik}\sum_{j=0}^d x_{ij}w_j\right) \\
						&= \sum_{i=1}^n -2x_{ik}y_i + \sum_{i=1}^n \left(2x_{ik}\sum_{j=0}^d x_{ij}w_j\right)
						\end{align*}\]
					</section>

					<section>
						Note: looking for a local optima, so set derivative equal to zero
						\[\begin{align*}
							0 &= \sum_{i=1}^n -2x_{ik}y_i + \sum_{i=1}^n \left(2x_{ik}\sum_{j=0}^d x_{ij}w_j\right) \\
							\sum_{i=1}^n x_{ik}y_i &= \sum_{i=1}^n \left(x_{ik}\sum_{j=0}^d x_{ij}w_j\right)
						\end{align*}\]
					</section>

					<section>
						<h2>Next step is tough</h2>
						<ul>
							<li>If you look closely though, the entire thing is wrought with our lin alg "cheats"</li>
							<li>Goal: Put it back into a matrix</li>
							<li>Think about the geometry and how the matrices line up</li>
						</ul>
					</section>

					<section>
						<h2>Left Half</h2>
						$\sum_{i=1}^n x_{ik}y_i = \sum_{i=1}^n \left(x_{ik}\sum_{j=0}^d x_{ij}w_j\right)$
						<ul>
							<li>First, let's look just at the left half, $\sum_{i=1}^n x_{ik}y_i$</li>
							<li>Think about the matrix interpretation of this operation: what is $i$?</li>
							<li>How is $i$ interpretted geometrically on $X$? $Y$?</li>
						</ul>
					</section>

					<section>
						<h2>Lin Alg Refresher</h2>
						Given $x,y \in \mathbb{R}^n = \mathbb{R}^{n \times 1}$ two column vectors, $z \in \mathbb{R}^{1 \times n}$ a row vector
						<br>
						<ul>
							<li>Inner product: dot product between two column vectors (DOWN $x$, and DOWN $y$)</li>
							<ul>
								<li>$\langle x, y\rangle = x^T \cdot y = x^Ty  \in \mathbb{R}^{1 \times 1}$</li>
							</ul>
							<li>Dot product between a row vector and a column vector (ACROSS $z$, and DOWN $x$)</li>
							<ul>
								<li>$z\cdot x = zx \in \mathbb{R}^{1 \times 1}$</li>
								</ul>
						</ul>
					</section>

					<section>
						<h2>Left Half</h2>
						$\sum_{i=1}^n x_{ik}y_i = \sum_{i=1}^n \left(x_{ik}\sum_{j=0}^d x_{ij}w_j\right)$
						\[\begin{align*}
							X &= \begin{bmatrix}
						1 & x_{11} & x_{12} & ... & x_{1d} \\
						\vdots & \vdots & \vdots & \ddots & \vdots \\
						1 & x_{n1} & x_{n2} & ... & x_{nd} \\
						\end{bmatrix}, Y = \begin{bmatrix}
						y_1 \\ \vdots \\ y_n
						\end{bmatrix}
						\end{align*}\]
						<ul>
							<li>notice that $i=1...n$ goes DOWN the training examples $x_k$ for the $k^{th}$ feature</li>
							<li>notice that $i=1...n$ goes DOWN the column of $Y$</li>
							<li>Since $x_k, Y$ are both vectors, this is just an inner product!</li>
						</ul>
					</section>

					<section>
						<h2>Simplified Left Half</h2>
						$\sum_{i=1}^n x_{ik}y_i$
						\[\begin{align*}
						x_k &= \begin{bmatrix}
						x_{1k}\\ \vdots \\ x_{nk}
						\end{bmatrix}, Y = \begin{bmatrix}
						y_1 \\ \vdots \\ y_n
						\end{bmatrix}
						\end{align*}\]
						<ul>
							<li>Geometrically, our sum is just $x_{1k}y_1 + ... + x_{nk}y_n$</li>
							<li>So, this is equivalent to $\langle x_k, Y\rangle = x_k^TY$</li>
						</ul>
					</section>

					<section>
						<h2>Right Half part 1</h2>
						$x_k^TY = \sum_{i=1}^n \left(x_{ik}\sum_{j=0}^d x_{ij}w_j\right)$
						\[\begin{align*}
						X &= \begin{bmatrix}
						1 & x_{11} & x_{12} & ... & x_{1d} \\
						\vdots & \vdots & \vdots & \ddots & \vdots \\
						1 & x_{n1} & x_{n2} & ... & x_{nd} \\
						\end{bmatrix}, w = \begin{bmatrix}
						w_0 \\ \vdots \\ w_d
						\end{bmatrix}
						\end{align*}\]
						<ul>
							<li>What is $j$ doing here on $x_i$?</li>
							<li>Going ACROSS the features of $i^{th}$ training example</li>
							<li>$x_i = \begin{bmatrix}1 & x_{i1} & x_{i2} & ... x_{id}\end{bmatrix}$</li>
							<li>Going DOWN the feature coefficients of $w$</li>
							<li>This is just a simple dot product: $x_i w$</li>
						</ul>
					</section>
					<section>
						<h2>Right Half part 2</h2>
						$x_k^TY = \sum_{i=1}^n \left(x_{ik}x_iw\right)$
						\[\begin{align*}
						X &= \begin{bmatrix}
						1 & x_{11} & x_{12} & ... & x_{1d} \\
						\vdots & \vdots & \vdots & \ddots & \vdots \\
						1 & x_{n1} & x_{n2} & ... & x_{nd} \\
						\end{bmatrix}, w = \begin{bmatrix}
						w_0 \\ \vdots \\ w_d
						\end{bmatrix}
						\end{align*}\]
						<br>
						<ul>
							<li>What does $x_iw$ look like over all $i$?</li>
							<li>Easy: This is just column vector $Xw$, a vector where the $i^{th}$ element is $x_iw$</li>
						</ul>
						$x_k^TY = \sum_{i=1}^n \left(x_{ik}(Xw)_i\right)$
					</section>

					<section>
						<h2>Right half part 3</h2>
						\[\begin{align*}
						X &= \begin{bmatrix}
						1 & x_{11} & x_{12} & ... & x_{1d} \\
						\vdots & \vdots & \vdots & \ddots & \vdots \\
						1 & x_{n1} & x_{n2} & ... & x_{nd} \\
						\end{bmatrix}, Xw = \begin{bmatrix}
						x_1w \\ \vdots \\ x_nw
						\end{bmatrix}
						\end{align*}\]
						<ul>
							<li>What is $i$ doing on $x_{ik}$? Going DOWN the training examples $x_k$ for the $k^{th}$ feature</li>
							<li>What is $i$ doing on $(Xw)_i$, the $i^{th}$ element of vector $Xw$?</li>
							<li>Going DOWN the $i^{th}$ training example</li>
							<li>Another inner product: just $\langle x_k, Xw\rangle = x_k^TXw$</li>
						</ul>
					</section>

					<section>
						<h2>Putting it together</h2>
						\[\begin{align*}
							x_k^TY &= x_k^TXw
						\end{align*}\]
						<ul>
							<li>just stack the $x_k$s into another matrix</li>
							<li>Remember: $k$ goes over columns (features) of $X$</li>
							\[\begin{align*}
								X_{\textrm{stacked}} &= \begin{bmatrix}
									\vdash x_0\dashv \\ \vdots \\ \vdash x_d \dashv
								\end{bmatrix} = \begin{bmatrix}
								1 & ... & 1 \\ \vdots & \ddots & \vdots \\ x_{1d} & ... & x_{nd}
								\end{bmatrix}
								\end{align*}\]
							<li>But... $X_{\textrm{stacked}}$ is just $X^T$</li>
						</ul>

					</section>

					<section>
						<h2>Solution</h2>
						\[\begin{align*}
							X^TY &= X^TXw \\
							\hat{w} &= (X^TX)^{-1}X^TY
						\end{align*}\]
						<ul>
							<li>Now let's do it again but with matrices...</li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h2>Matrix Derivation</h2>
						<ul>
							<li>This one is a little less intuitive, but once you see it, is MUCH simpler</li>
						</ul>
					</section>


					<section>
						\[\begin{align*}
							LS &= ||\epsilon||_2^2 = \langle \epsilon, \epsilon \rangle = \epsilon^T\epsilon \\
								&= (Y - Xw)^T (Y-Xw)
						\end{align*}\]
						Remember to distribute the transpose linearly and reverse your terms as you transpose...
						\[\begin{align*}
								&= \left(Y^T - w^TX^T\right)(Y - Xw) \\
								&= Y^TY - w^TX^TY - Y^TXw + w^TX^TXw
						\end{align*}\]
					</section>


					<section>
						<ul>
							<li>Remember this is the SUM of SQUARED RESIDUALS/ERROR</li>
							<li>SUM means it is scalar!</li>
							<li>Then these things summing to equal LS are scalar. so $Y^TY, -w^TX^TY, - Y^TXw,w^TX^TXw$ are ALL also scalars</li>
							<li>Means then are all $\in \mathbb{R}$</li>
							<li>If $x \in \mathbb{R}$, $x=x^T$!</li>
							<li>So... $-w^TX^TY = (-w^TX^TY)^T = Y^TXw$</li>
						</ul>
					</section>

					<section>
						\[\begin{align*}
							LS &= Y^TY - w^TX^TY - Y^TXw + w^TX^TXw  \\
							&= Y^TY - 2Y^TXw + w^TX^TXw \\
							\frac{\delta LS}{\delta w} &= -2(Y^TX)^T + 2X^TXw = 0 \\
							X^TY &= X^TXw
						\end{align*}\]
						<ul>
							<li>note: $\frac{\delta [Xw]}{\delta w} = X^T$</li>
							<li>$\frac{\delta [w^TX^TXw]}{\delta w} = 2X^TXw$</li>
						</ul>
					</section>


					<section>
						<h2>Solution</h2>
						\[\begin{align*}
							X^TY &= X^TXw \\
							\hat{w} &= (X^TX)^{-1}X^TY
						\end{align*}\]
						<ul>
							<li>Note: Same solution as before; far fewer steps!</li>
						</ul>
					</section>
				</section>
				<section>
					<h2>Prediction</h2>
					<ul>
						<li>Given: $X_{new} \in \mathbb{R}^{d+1}$, a new example</li>
						<li>Trained $\hat{w}$ using the closed form solution we just derived</li>
						<li>$\hat{Y} = X_{new}\hat{w}$</li>
					</ul>
				</section>
				<section>
					<section>
						<h1>Code Demo</h1>
					</section>
					<section data-markdown>
						<script type="text/template">
							## Producing X vector
							```
							%matplotlib inline  # allows matplotlib to plot in jupyter-notebook
							import numpy as np
							import matplotlib.pyplot as plt

							n = 100  # the number of samples
							d = 1  # the number of dimensions of our points
							X = np.linspace(0, n-1, n).reshape(n, d)  # now, X is just a nx1 matrix
							X.shape  # returns (100, 1)

							X = np.hstack(((np.ones((n, 1)), X)))  # adds a column of 1s for constant term
							X.shape  # returns (100, 2) because we have a column of 1s
							```
						</script>
					</section>

					<section data-markdown>
						<script type="text/template">
							## Producing and Plotting Simulated Responses
							```
							covar = 200*np.identity(d)  # simulated data with 200*I covariance
							# simulate n samples of zero-mean, gaussian noise, just like the model
							# specifies
							E = np.random.multivariate_normal(mean=np.zeros((d,)), cov=covar, size=n)
							W = np.array([50, .5]).reshape(d+1, 1)
							Y = X.dot(W) + E  # observations with error

							fig = plt.figure()
							ax = fig.add_subplot(1,1,1)
							ax.scatter(X[:,1], Y)  # plot non-constant X terms
							ax.set_xlabel('x')
							ax.set_ylabel('y')
							ax.set_title('Simulated Data')
							fig.tight_layout()  # makes sure we don't cutoff anything
							fig.show()
							```
						</script>
					</section>

					<section>
						<h2>Example Simulated Data Plot</h2>
						<img src="../img/linear_task.png">
					</section>
                    <section>
                        <h2>HW1 Jupyter Notebook Setup</h2>
                        <ul>
                            <li>Install <a href="http://www.h5py.org/">H5py</a></li>
                            <li><a href="https://nbviewer.jupyter.org/github/ebridge2/cs_475_spring_2017/blob/master/python-tutorial.ipynb">basic python tutorial</a></li>
                            <li><a href="https://nbviewer.jupyter.org/github/ebridge2/cs_475_spring_2017/blob/master/PS1-asymloss-notebook.ipynb">hw1-setup-tutorial</a> (needs H5py and jupyter-notebook)</li>
                        </ul>
                        
                    </section>
                </section>
            </div>

		</div>

		<script src="../reveal.js/lib/js/head.min.js"></script>
		<script src="../reveal.js/js/reveal.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				math: {
					// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
					config: 'TeX-AMS_HTML-full'
				},

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: '../reveal.js/plugin/math/math.js', async: true },
					{ src: '../reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '../reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '../reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '../reveal.js/plugin/zoom-js/zoom.js', async: true },
					{ src: '../reveal.js/plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
